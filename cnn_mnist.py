# -*- coding: utf-8 -*-
"""CNN_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t2u7pjmAcKe6Q6yjjvlcQibN2e-W-b3e
"""

import tensorflow as tf

from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)

mnist.train.images.shape

28**2

n_input=784
input_width=28
input_height=28
n_channel=1

n_conv1=32
n_conv2=64

conv1_k=5
conv2_k=5

n_hidden=1024
n_out=10

pooling_size=2

conv2_width=input_width//(pooling_size*pooling_size)
conv2_height=input_height//(pooling_size*pooling_size)

input_hidden=conv2_width*conv2_height*n_conv2

weights={
    
    'c1':tf.Variable(tf.random_normal([conv1_k,conv1_k,n_channel,n_conv1])),
    'c2':tf.Variable(tf.random_normal([conv2_k,conv2_k,n_conv1,n_conv2])),
    'h1':tf.Variable(tf.random_normal([input_hidden,n_hidden])),
     'out':tf.Variable(tf.random_normal([n_hidden,n_out]))    
    
}

biases={
     'c1':tf.Variable(tf.random_normal([n_conv1])),
     'c2':tf.Variable(tf.random_normal([n_conv2])),
     'h1':tf.Variable(tf.random_normal([n_hidden])),
     'out':tf.Variable(tf.random_normal([n_out]))   
    
    
    
}

def conv2D(x,weights,biases,stride=1):
  conv=tf.nn.conv2d(x,weights,padding="SAME",strides=[1,stride,stride,1])
  out=tf.nn.bias_add(conv,biases)
  out=tf.nn.relu(out)
  return out

def maxpool(x,k=2):
  return tf.nn.max_pool(x,padding="SAME",ksize=[1,k,k,1],strides=[1,k,k,1])

def forward_propogation(x,weights,biases):
  x=tf.reshape(x,shape=[-1,input_width,input_height,n_channel])
  conv1=conv2D(x,weights['c1'],biases['c1'])
  conv1=maxpool(conv1,k=pooling_size)
  
  conv2=conv2D(conv1,weights['c2'],biases['c2'])
  conv2=maxpool(conv2,k=pooling_size)
  
  hidden_input=tf.reshape(conv2,shape=[-1,input_hidden])
  hidden_input=tf.add(tf.matmul(hidden_input,weights['h1']),biases['h1'])
  hidden_out=tf.nn.relu(hidden_input)
  
  out=tf.add(tf.matmul(hidden_out,weights['out']),biases['out'])
  return out

x=tf.placeholder("float",[None,n_input])
y=tf.placeholder(tf.int32,[None,n_out])

pred=forward_propogation(x,weights,biases)

cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y))

optimizer=tf.train.AdamOptimizer(learning_rate=0.01)
optimize_step=optimizer.minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

batch_size = 100
for i in range(25):
    num_batches = int(mnist.train.num_examples/batch_size)
    total_cost = 0
    for j in range(num_batches):
        batch_x, batch_y = mnist.train.next_batch(batch_size) 
        c, _ = sess.run([cost, optimize_step], feed_dict={x:batch_x, y:batch_y})
        total_cost += c
    print(total_cost)

predictions=tf.argmax(pred,axis=1)
actual_labels=tf.argmax(y,axis=1)
correct_predictions=tf.equal(predictions,actual_labels)
correct=sess.run(correct_predictions,feed_dict={x:mnist.test.images,y:mnist.test.labels})

mnist.test.images.shape[0]

correct.sum()/mnist.test.images.shape[0]

